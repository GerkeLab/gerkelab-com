---
title: When does {dtplyr} help me?
description: Investigating where efficiency gains occur when ingesting flat files with dtplyr
author: 
  - "Travis Gerke"
date: '2019-11-14'
slug: dtplyr-and-fread
categories:
  - R
tags:
  - Tips
hero_bg: "/img/hero/ethan-weil-528367-unsplash.jpg"
hero_credit: '[Ethan Weil](https://unsplash.com/@weilstyle?utm_medium=referral&amp;utm_campaign=photographer-credit).'
---
  
[dtplyr]: https://dtplyr.tidyverse.org/
[readr]: https://readr.tidyverse.org
[datatable]: https://rdatatable.gitlab.io/data.table/
[data]: https://www.kaggle.com/jameslko/gun-violence-data/data

This post explores where to expect efficiency gains when using the new [[dtplyr]]{.pkg} to import and manipulate large flat files.

```{r library, echo=FALSE}
library(data.table)
library(dtplyr)
library(tidyverse)
library(microbenchmark)

#prevent printing of readr column specs
options(readr.num_columns = 0)
```


```{r install, eval=FALSE}
# Install packages if you need to
install.packages(c("tidyverse", "fs"))

<<library>>
```

## Problem

This week, we got the following exciting announcement from Hadley Wickham regarding a big [[dtplyr]]}{.pkg} release!

<div style="width: 100%; display: flex; align-content: center;">
  <div style="margin: auto;">
  <img src="/blog/2019-11-14-dtplyr-and-fread/dtplyr-tweet.png" style="max-width: 600px" />
  </div>
</div>

When dealing with large flat files, I have often resorted to `data.table`'s `fread()` function, which is a very fast alternative to `readr`'s `read_csv`. Unfortunately, I'm not too comfortable with `data.table` syntax for data munging, so I have a few ugly pipelines laying around where I mash data from `fread` into some `tibble`-ish format that accepts `dplyr` verbs. In this setting [[dtplyr]]}{.pkg} <i>feels</i> like the idyllic solution but, being a lesser mortal than Hadley, I didn't immediately connect all the dots. 

Specifically, does [[dtplyr]]}{.pkg} let me avoid `fread` altogether? If not, does the main [[dtplyr]]}{.pkg} function `lazy_dt()` still give me efficiency gains when I've loaded something from `fread`? Lastly, does `lazy_dt()` help when I've loaded something fully into memory via `readr`?

## Example Data

To illustrate, we'll use a modest 150MB csv [dataset provided by the Gun Violence Archive and available in Kaggle][data] which reports over 260k gun violence incidents in the US between 2013 and 2018. Note that we don't directly repost the data here in accordance with use agreements; if you'd like to reproduce the below, please download the csv via the above link and stuff it into your working directory. 

## Benchmarks

### Using `read_csv`

```{r}
microbenchmark(
   read_csv("gun-violence-data_01-2013_03-2018.csv", progress = FALSE) %>% 
      group_by(state) %>%
      count(sort = TRUE) %>%
      print(),
   times = 1,
   unit = "s"
)$time/1e9
```

### Using `fread`

```{r}
microbenchmark(
   fread("gun-violence-data_01-2013_03-2018.csv") %>% 
      lazy_dt() %>%
      group_by(state) %>%
      count(sort = TRUE) %>% 
      as_tibble() %>% 
      print(),
   times = 1, 
   unit = "s"
)$time/1e9
```

A lot of the above improvement likely due to speed improvements with `fread`. Does `lazy_dt()` still save us time when data are already in memory?

```{r}
dat_readr <- read_csv("gun-violence-data_01-2013_03-2018.csv", progress = FALSE) 
microbenchmark(
   dat_readr %>% 
      group_by(state) %>%
      count(sort = TRUE),
   times = 100
)
```

```{r}
microbenchmark(
   dat_readr %>% 
      lazy_dt() %>%
      group_by(state) %>%
      count(sort = TRUE),
   times = 100
)
```

Here's the same example using `fread`.
```{r}
dat_dt <- fread("gun-violence-data_01-2013_03-2018.csv")
microbenchmark(
   dat_dt %>% 
      lazy_dt() %>%
      group_by(state) %>%
      count(sort = TRUE) %>% 
      as_tibble(),
   times = 100
) 
```